# -*- coding: utf-8 -*-
"""Hotel Room Cancellations Project - Haikal Riza.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1No4i5Q96FfBebAEFuCFX5LLudMXwY5qx

# Hotel Bookings

## Import Data & Libraries
"""

# Import the libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import gdown

# Visualization imports
import plotly.express as px

# Import the Hotel Reservations dataset
booking_url = "https://docs.google.com/spreadsheets/d/1HqgL4jEcfvgj_xdVqpluRwWbliuscvkBiLdmsnVx7xg/edit?gid=1742145946#gid=1742145946"
booking_csv = booking_url.replace('/edit?gid=', '/export?format=csv&gid=')
df = pd.read_csv(booking_csv)
booking_df = df.copy()

booking_df.head()

"""## Data Preparation & Data Cleaning

### Looking into data type of each column and seeing if there is any null data
"""

# Looking into each column data type and null data
booking_df.info()

# There is no null data and need to convert booking_status column into boolean and arrival_year, arrival_month, and arrival_date into datetime data type

"""There is no missing value / null

### Duplicated Data
"""

# Checking if there is any duplicated Booking ID
booking_df[booking_df['Booking_ID'].duplicated() == True]

# There is no duplicated Booking ID

"""### Arrival Date, Month, and Year"""

# Converting arrival date, month, and year into one date-time column
booking_df['arrival_time'] = pd.to_datetime(dict(
    year=booking_df['arrival_year'],
    month=booking_df['arrival_month'],
    day=1))

booking_df[['arrival_year', 'arrival_month', 'arrival_date', 'arrival_time']]

# Looking at the data that has been added with the date column that has been transformed into datetime type
booking_df.info()

"""### Target feature conversion into Boolean data type"""

# Converting booking status column data type into boolean, as the booking status describing whether or not the booking is cancelled
booking_df['cancellation_status'] = booking_df['booking_status'].replace({'Not_Canceled': 0, 'Canceled': 1}).astype(int)
booking_df.info()

"""## Understand the Dataset & Table Relationship

### Numerical Columns
"""

# checking the numerical feature statistic description
booking_df.describe()

# Looking at the numerical features correlation coefficient with each other
features_num = ['no_of_adults', 'no_of_children', 'no_of_weekend_nights', 'no_of_week_nights', 'required_car_parking_space', 'lead_time', 'repeated_guest', 'no_of_previous_cancellations', 'no_of_previous_bookings_not_canceled', 'avg_price_per_room', 'no_of_special_requests']

book_corr = round(booking_df[features_num].corr(), 2)
sns.heatmap(book_corr, annot=True)

# Looking at the relationship of the numerical features
sns.pairplot(booking_df[features_num])

"""### Categorical Columns"""

# Listing categorical features
features_cat = ['type_of_meal_plan', 'room_type_reserved', 'market_segment_type']

# Looking at the description of each categorical features
booking_df[features_cat].describe()

# Proportion of the categorical feature Type of Meal Plan
booking_df['type_of_meal_plan'].value_counts(normalize=True)

# Value counts of the categorical feature Type of Meal Plan
booking_df['type_of_meal_plan'].value_counts().plot(kind='bar')
plt.show()

# Proportion of the categorical feature Room Type Reserved
booking_df['room_type_reserved'].value_counts(normalize=True)

# Value counts of the categorical feature Room Type Reserved
booking_df['room_type_reserved'].value_counts().plot(kind='bar')
plt.show()

# Proportion of the categorical feature Market Segment Type
booking_df['market_segment_type'].value_counts(normalize=True)

# Value counts of the categorical feature Market Segment Type
booking_df['market_segment_type'].value_counts().plot(kind='bar')
plt.show()

# Mean score summary for each numerical features
features_num_cancellation_mean = round(booking_df[features_num].mean(), 2)
features_num_cancellation_mean

"""## Exploratory Data Analysis

### Revenue
"""

# Cancelled Booking
booking_cancelled = booking_df[booking_df['booking_status'] == 'Canceled']

# Not Canceled Booking
booking_not_cancelled = booking_df[booking_df['booking_status'] == 'Not_Canceled']
booking_not_cancelled.head()

# Revenue in 2018
booking_not_cancelled_2018 = booking_not_cancelled[booking_not_cancelled['arrival_year'] == 2018]
revenue_2018 = sum(booking_not_cancelled_2018['avg_price_per_room'] * ((booking_not_cancelled_2018['no_of_week_nights'] + booking_not_cancelled_2018['no_of_weekend_nights'])))
(print(f"Revenue in 2018: ${round(revenue_2018, 2)}"))

# Revenue Loss
booking_cancelled_2018 = booking_cancelled[booking_cancelled['arrival_year'] == 2018]
revenue_loss_2018 = sum(booking_cancelled_2018 ['avg_price_per_room'] * ((booking_cancelled_2018 ['no_of_week_nights'] + booking_cancelled_2018 ['no_of_weekend_nights'])))
(print(f"Revenue Loss in 2018: ${round(revenue_loss_2018, 2)}"))

"""### Cancellation Rate"""

# Overall Cancellation Rate
cancellation_rate = booking_cancelled['Booking_ID'].nunique() / booking_df['Booking_ID'].nunique()
print(f"Cancellation Rate: {round(cancellation_rate, 4) * 100}{'%'}")

# Cancellation Rate in 2017
cancellation_rate_2017 = (booking_cancelled[booking_cancelled['arrival_year'] == 2017]['Booking_ID'].nunique()) / (booking_df[booking_df['arrival_year'] == 2017]['Booking_ID'].nunique())
print(f"Cancellation Rate in 2017: {round(cancellation_rate_2017, 4) * 100}{'%'}")

# Cancellation Rate in 2018
cancellation_rate_2018 = (booking_cancelled[booking_cancelled['arrival_year'] == 2018]['Booking_ID'].nunique()) / (booking_df[booking_df['arrival_year'] == 2018]['Booking_ID'].nunique())
print(f"Cancellation Rate in 2018: {round(cancellation_rate_2018, 4) * 100}{'%'}")

# Checking the number of monthly booking
monthly_booking = booking_df.groupby('arrival_time')['Booking_ID'].nunique()
monthly_booking = monthly_booking.sort_index()
monthly_booking

# Monthly booking counts
monthly_booking.plot(kind='bar')

# Seeing the monthly Cancellation count
monthly_cancellation_count = booking_cancelled.groupby('arrival_time')['Booking_ID'].nunique()
monthly_cancellation_count

# Seeing the monthly Cancellation Rate
monthly_cancellation_rate = booking_cancelled.groupby('arrival_time')['Booking_ID'].nunique() / booking_df.groupby('arrival_time')['Booking_ID'].nunique()

monthly_cancellation_rate

# Monthly cancellation rate
monthly_cancellation_rate.plot(kind='bar')

"""### Categorical data comparison between Canceled bookings and Non-Canceled bookings"""

# Looking at the difference of type of meal plan between Canceled bookings and Non-Canceled bookings
type_of_meal_plan_diff = round(pd.crosstab(booking_df['type_of_meal_plan'], booking_df['booking_status'], normalize='columns'), 2)
type_of_meal_plan_diff["Diff"] = type_of_meal_plan_diff['Canceled'] - type_of_meal_plan_diff['Not_Canceled']
type_of_meal_plan_diff

# Looking at the difference of type of room reserved between Canceled bookings and Non-Canceled bookings
room_type_reserved_diff = round(pd.crosstab(booking_df['room_type_reserved'], booking_df['booking_status'], normalize='columns'), 2)
room_type_reserved_diff["Diff"] = room_type_reserved_diff["Canceled"] - room_type_reserved_diff["Not_Canceled"]
room_type_reserved_diff

# Looking at the difference of market segment type plan between Canceled bookings and Non-Canceled bookings
market_segment_diff = round(pd.crosstab(booking_df['market_segment_type'], booking_df['booking_status'], normalize='columns'), 2)
market_segment_diff["Diff"] = market_segment_diff["Canceled"] - market_segment_diff["Not_Canceled"]
market_segment_diff

# Looking at the difference of market segment type plan between Canceled bookings and Non-Canceled bookings
market_segment_diff_2 = round(pd.crosstab(booking_df['booking_status'], booking_df['market_segment_type'], normalize='columns'), 2)
market_segment_diff_2

# Looking at the difference of arrival month between Canceled bookings and Non-Canceled bookings
arrival_month_diff = round(pd.crosstab(booking_df['arrival_month'], booking_df['booking_status'], normalize='columns'), 2)
arrival_month_diff["Diff"] = arrival_month_diff["Canceled"] - arrival_month_diff["Not_Canceled"]
arrival_month_diff

"""### Numerical data comparison between Canceled bookings and Non-Canceled bookings"""

# Create boxplots for all numerical features, comparing canceled and non-canceled bookings

# Determine the number of rows and columns for subplots
num_features = len(features_num)
num_cols = 4  # You can adjust this as needed
num_rows = (num_features + num_cols - 1) // num_cols

fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols * 5, num_rows * 4))
axes = axes.flatten() # Flatten the 2D array of axes for easy iteration

for i, feature in enumerate(features_num):
    ax = axes[i]
    ax.boxplot([booking_df[feature], booking_cancelled[feature], booking_not_cancelled[feature]])
    ax.set_xticklabels(['Total', 'Cancelled', 'Not Cancelled'])
    ax.set_title(f'{feature.replace("_", " ").title()}')

    # Set y-axis limit for 'no_of_weekend_nights' specifically if needed
    if feature == 'no_of_weekend_nights':
        ax.set_ylim(top=17.5) # Set dynamically for the feature
    elif feature == 'no_of_adults':
        ax.set_ylim(bottom=-0.5) # Example: to ensure 0 is visible and not cut
    elif feature == 'no_of_children':
        ax.set_ylim(bottom=-0.5) # Example: to ensure 0 is visible and not cut

# Hide any unused subplots
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

# Looking at mean score difference of numerical features between Canceled bookings and Non-Canceled bookings
features_num_mean_diff = round(booking_df.groupby('cancellation_status')[features_num].mean(), 2)
features_num_mean_diff = features_num_mean_diff.transpose()
features_num_mean_diff['Diff'] = features_num_mean_diff[1] - features_num_mean_diff[0]
features_num_mean_diff

# Create boxplots for Lead Time
fig, ax = plt.subplots()
ax.boxplot([booking_df['lead_time'], booking_cancelled['lead_time'], booking_not_cancelled['lead_time']])
ax.set_xticklabels(['Total', 'Cancelled', 'Not Cancelled'])

plt.tight_layout()
plt.show()

# Create boxplots for Avg. Price per Room
fig, ax = plt.subplots()
ax.boxplot([booking_df['avg_price_per_room'], booking_cancelled['avg_price_per_room'], booking_not_cancelled['avg_price_per_room']])
ax.set_xticklabels(['Total', 'Cancelled', 'Not Cancelled'])

plt.tight_layout()
plt.show()

# Create boxplots for No. of Special Request
fig, ax = plt.subplots()
ax.boxplot([booking_df['no_of_special_requests'], booking_cancelled['no_of_special_requests'], booking_not_cancelled['no_of_special_requests']])
ax.set_xticklabels(['Total', 'Cancelled', 'Not Cancelled'])

plt.tight_layout()
plt.show()

# Create boxplots for No. of Weekend Nights
fig, ax = plt.subplots()
ax.boxplot([booking_df['no_of_weekend_nights'], booking_cancelled['no_of_weekend_nights'], booking_not_cancelled['no_of_weekend_nights']])
ax.set_xticklabels(['Total', 'Cancelled', 'Not Cancelled'])

plt.tight_layout()
plt.show()

# Create boxplots for No. of Weekday Nights
fig, ax = plt.subplots()
ax.boxplot([booking_df['no_of_week_nights'], booking_cancelled['no_of_week_nights'], booking_not_cancelled['no_of_week_nights']])
ax.set_xticklabels(['Total', 'Cancelled', 'Not Cancelled'])

plt.tight_layout()
plt.show()

# Create boxplots for No. of Adults
fig, ax = plt.subplots()
ax.boxplot([booking_df['no_of_adults'], booking_cancelled['no_of_adults'], booking_not_cancelled['no_of_adults']])
ax.set_xticklabels(['Total', 'Cancelled', 'Not Cancelled'])

plt.tight_layout()
plt.show()

# Create boxplots for No. of Children
fig, ax = plt.subplots()
ax.boxplot([booking_df['no_of_children'], booking_cancelled['no_of_children'], booking_not_cancelled['no_of_children']])
ax.set_xticklabels(['Total', 'Cancelled', 'Not Cancelled'])

plt.tight_layout()
plt.show()

# Create boxplots for No. of Required Car Parking
fig, ax = plt.subplots()
ax.boxplot([booking_df['required_car_parking_space'], booking_cancelled['required_car_parking_space'], booking_not_cancelled['required_car_parking_space']])
ax.set_xticklabels(['Total', 'Cancelled', 'Not Cancelled'])

plt.tight_layout()
plt.show()

# Create boxplots for Repeated Guest
fig, ax = plt.subplots()
ax.boxplot([booking_df['repeated_guest'], booking_cancelled['repeated_guest'], booking_not_cancelled['repeated_guest']])
ax.set_xticklabels(['Total', 'Cancelled', 'Not Cancelled'])

plt.tight_layout()
plt.show()

# Create boxplots for No. of Previous Cancellations
fig, ax = plt.subplots()
ax.boxplot([booking_df['no_of_previous_cancellations'], booking_cancelled['no_of_previous_cancellations'], booking_not_cancelled['no_of_previous_cancellations']])
ax.set_xticklabels(['Total', 'Cancelled', 'Not Cancelled'])

plt.tight_layout()
plt.show()

# Create boxplots for No. of Previous Cancellations
fig, ax = plt.subplots()
ax.boxplot([booking_df['no_of_previous_bookings_not_canceled'], booking_cancelled['no_of_previous_bookings_not_canceled'], booking_not_cancelled['no_of_previous_bookings_not_canceled']])
ax.set_xticklabels(['Total', 'Cancelled', 'Not Cancelled'])

plt.tight_layout()
plt.show()

# Mean score for each numerical features
features_num_cancellation_mean = round(booking_df[features_num].mean(), 2)
features_num_cancellation_mean

"""## Statistical Analysis & Visualizations

### Hypothesis Testing
"""

# Import library for hypothesis testing using scipy
from scipy.stats import chi2_contingency # for chi-square test
from scipy.stats import ttest_ind # for t-test
!pip install pingouin
import pingouin

"""#### Online Booking"""

# Creating boolean column of online booking if the customer booked via online
copy_booking_df = booking_df.copy()
online_booking_df = pd.get_dummies(booking_df['market_segment_type'], dtype=bool)
copy_online_booking_df = pd.concat((copy_booking_df, online_booking_df), axis=1)
copy_online_booking_df

# Online booking cancellation hypothesis testing using Chi2 Square
pingouin.chi2_independence(data=copy_online_booking_df, x='Online', y='cancellation_status')

"""With p-value is 3.8e-91 (below 0.05), the difference in cancellation between the online booking and non-online booking is statistically different (accept H1). As the cancellation rate proportion of the online booking is higher, it means online booking has the higher chance to be cancelled.

#### Lead Time
"""

# Cancelled booking lead time
cancel_lead_time = booking_df[booking_df['cancellation_status'] == 1]["lead_time"]
cancel_lead_time_mean = round(cancel_lead_time.mean(), 2)
print(f"Cancelled Booking Lead Time: {cancel_lead_time_mean }")

# Not Cancelled boking lead time
not_cancel_lead_time = booking_df[booking_df['cancellation_status'] == 0]["lead_time"]
not_cancel_lead_time_mean = round(not_cancel_lead_time.mean(), 2)
print(f"Non-Cancelled Booking Lead Time: {not_cancel_lead_time_mean }")

# Lead time difference
lead_time_diff = cancel_lead_time_mean - not_cancel_lead_time_mean
lead_time_diff_rate = round((lead_time_diff / not_cancel_lead_time_mean) * 100, 2)
print(f"Lead Time Difference (%): {lead_time_diff_rate}")

# Hypothesis testing of Lead TIme
pingouin.ttest(x=cancel_lead_time, y=not_cancel_lead_time, paired=False, alternative='greater')

"""With p-value is 9e-05 (below 0.05), the difference in lead time between the canceled booking and non-canceled booking is statistically different (accept H1). As the mean score of lead time for cancelled booking is higher, it means the longer lead time between booking time and arrival time, the more likely it is to be cancelled.

#### Average Price per Room
"""

# Cancelled booking average price per room
cancel_avg_price = booking_df[booking_df['cancellation_status'] == 1]["avg_price_per_room"]
cancel_avg_price_mean = round(cancel_avg_price.mean(), 2)
print(f"Cancelled Booking Average Price per Room: {cancel_avg_price_mean }")

# Not Cancelled boking average price per room
not_cancel_avg_price = booking_df[booking_df['cancellation_status'] == 0]["avg_price_per_room"]
not_cancel_avg_price_mean = round(not_cancel_avg_price.mean(), 2)
print(f"Non-Cancelled Average Price per Room: {not_cancel_avg_price_mean }")

# Average Price per Room difference
avg_price_diff = cancel_avg_price_mean - not_cancel_avg_price_mean
avg_price_diff_rate = round((lead_time_diff / not_cancel_avg_price_mean) * 100, 2)
print(f"Average Price per Room Difference (%)): {avg_price_diff_rate}")

# Hypothesis testing of Avg. Price per Room
pingouin.ttest(x=cancel_avg_price, y=not_cancel_avg_price, paired=False, alternative='greater')

"""With p-value is 1.17e-175 (below 0.05), the difference in avg. price per room between the canceled booking and non-canceled booking is statistically different (accept H1). As the mean score of avg. price per room for cancelled booking is higher, it means the higher price they paid, the more likely the booking to be cancelled.

#### Number of Weekday Nights
"""

# Cancelled booking number of week nights
cancel_no_of_week_nights = booking_df[booking_df['cancellation_status'] == 1]["no_of_week_nights"]
cancel_no_of_week_nights_mean = round(cancel_no_of_week_nights.mean(), 2)
print(f"Cancelled Booking Number of Week Nights: {cancel_no_of_week_nights_mean }")

# Not Cancelled boking average price per room
not_cancel_no_of_week_nights = booking_df[booking_df['cancellation_status'] == 0]["no_of_week_nights"]
not_cancel_no_of_week_nights_mean = round(not_cancel_no_of_week_nights.mean(), 2)
print(f"Non-Cancelled Number of Week Nights: {not_cancel_no_of_week_nights_mean }")

# Number of Weekday Nights difference
no_of_week_nights_diff = cancel_no_of_week_nights_mean - not_cancel_no_of_week_nights_mean
no_of_week_nights_diff_rate = round((no_of_week_nights_diff / not_cancel_no_of_week_nights_mean) * 100, 2)
print(f"Number of Week Nights Difference (%)): {no_of_week_nights_diff_rate}")

# Hypothesis testing of Number of Week Nights
pingouin.ttest(x=cancel_no_of_week_nights, y=not_cancel_no_of_week_nights, paired=False, alternative='greater')

"""With p-value is 2.72e-62 (below 0.05), the difference in number of weekday nights between the canceled booking and non-canceled booking is statistically different (accept H1). As the mean score of number of weeday nights for cancelled booking is higher, it means bookings with more weekday nights have higher chances to be cancelled.

#### Number of Special Request
"""

# Crosstabing between cancellation status and number of special request
no_of_special_request_cancel = pd.crosstab(booking_df['cancellation_status'], booking_df['no_of_special_requests'])
no_of_special_request_cancel

# Crosstabing between cancellation status and number of special request
no_of_special_request_cancel_proportion = pd.crosstab(booking_df['cancellation_status'], booking_df['no_of_special_requests'], normalize='columns')
no_of_special_request_cancel_proportion

# Online booking cancellation hypothesis testing using Chi2 Square
no_of_special_request_cancel_test = chi2_contingency(no_of_special_request_cancel)
print(no_of_special_request_cancel_test)

"""With p-value is 3.8e-91 (below 0.05), the difference in number of special request between the online booking and non-online booking is statistically different (accept H1). As the cancellation rate proportion of less special request is lower, it means the lesser the booking has special request, the higher the chance it is to be cancelled."""

# Hypotestical testing of number of special request  impacting cancellation status with Chi-Square test using pingouin library
excpected, observed, stats = pingouin.chi2_independence(data=booking_df, x='no_of_special_requests', y='cancellation_status')
print(stats[stats['test'] == 'pearson'])

# Hypotestical testing of number of special request  impacting cancellation status with anova test using pingouin library
pingouin.anova(data=booking_df,
               dv='cancellation_status',
               between='no_of_special_requests')

# Hypotestical testing of number of special request  impacting cancellation status with pairwise test using pingouin library
pingouin.pairwise_tests(data=booking_df,
                        dv='cancellation_status',
                        between='no_of_special_requests',
                        padjust='bonf')

"""### Predicting Model

#### Split the data into traning and test set
"""

# Imports ML configuration
from sklearn.model_selection import train_test_split, cross_validate, cross_val_score, learning_curve, validation_curve
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, precision_recall_fscore_support, average_precision_score, ConfusionMatrixDisplay
from sklearn import set_config
set_config(display="diagram")

# Listing categorical features
features_cat = ['type_of_meal_plan', 'room_type_reserved', 'market_segment_type', 'arrival_month']

# Combining categorical features and numerical features
booking_df_new = pd.concat([booking_df['Booking_ID'], booking_df[features_num], booking_df[features_cat], booking_df['cancellation_status']], axis=1)
booking_df_new = booking_df_new.set_index('Booking_ID')
booking_df_new.info()

# Defining features and target column
X = booking_df_new.drop(columns='cancellation_status', axis=1)
y = booking_df_new['cancellation_status']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20)

# Checking the training data of the X variables
X_train.head()

# Checking the test data of the X variables
X_test.head()

"""#### Preprocessing data for the model"""

# Preprocessing data for Decision Tree Classification model
transformer_num = SimpleImputer(strategy = 'constant')

transformer_cat = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy = 'constant', fill_value = "Unknown")),
    ("onehot", OneHotEncoder(handle_unknown = 'ignore'))
])

preprocessor = ColumnTransformer(transformers=[
    ("num", transformer_num, features_num),
    ("cat", transformer_cat, features_cat)
])

preprocessor

"""#### Decision Tree Classification Model"""

# Compose data preprocessing and model into a single pipeline
steps = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', DecisionTreeClassifier(min_samples_leaf=50, class_weight='balanced', random_state=20))
])

steps.fit(X_train, y_train)

# Predict the y variable using the Decision Tree Classification
y_pred = steps.predict(X_test)

# Checking the confusion metrics of the test dataset using Decision Tree Classification
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='g')
plt.xlabel('Predicted label')
plt.ylabel('Actual label')

# Checking the confusion metrics of the trining dataset using Decision Tree Classification
cm_train = confusion_matrix(y_train, steps.predict(X_train))
sns.heatmap(cm_train, annot=True, fmt='g')
plt.xlabel('Predicted label')
plt.ylabel('Actual label')

# Checking the accuracy score of Decision Tree Classification using the test dataset
acc_test = accuracy_score(y_test, steps.predict(X_test))
print(f"Accuracy test: {acc_test}")

# Checking the accuracy score of Decision Tree Classification using the training dataset
acc_train = accuracy_score(y_train, steps.predict(X_train))
print(f"Accuracy train: {acc_train}")

# Checking the recall score of Decision Tree Classification using the test and training dataset
print(recall_score(y_test, steps.predict(X_test)))
print(recall_score(y_train, steps.predict(X_train)))

# Checking the precision score of Decision Tree Classification using the test dataset
from sklearn.metrics import average_precision_score

pr_auc = average_precision_score(y_test, steps.predict(X_test))
print(pr_auc)

"""#### Cross Validation to measure performance of the model"""

# Importing module for Cross Validation
from sklearn.model_selection import cross_val_score, learning_curve, validation_curve

# Cross Validation Score
CV_scores = cross_val_score(steps, X, y, scoring='average_precision', cv=5, n_jobs=-1)
print(f'Cross Validation score: {CV_scores.mean()}')

# Visualizing the Cross Validation score
train_sizes, train_scores, val_scores = learning_curve(steps, X, y, cv=10, scoring='accuracy', train_sizes=np.linspace(0.1, 1, 10))

train_mean = train_scores.mean(axis=1)
val_mean = val_scores.mean(axis=1)

plt.plot(train_sizes, train_mean, label='Training score')
plt.plot(train_sizes, val_mean, label='Validation score')
plt.xlabel('Training Set Size')
plt.ylabel('Accuracy Score')
plt.title('Learning Curve')
plt.legend(loc='lower right')
plt.ylim(0)
plt.show()

# Visualizing the Bias-Variance Tradeoff
param_range = range(1, 21)

train_scores, val_scores = validation_curve(steps, X, y, param_name='model__max_depth', param_range=param_range, cv=10, scoring='accuracy', n_jobs=-1)

plt.plot(param_range, train_scores.mean(axis=1), label='Training')
plt.plot(param_range, val_scores.mean(axis=1), label='Validation')
plt.xlabel('Max Depth')
plt.ylabel('Accuracy')
plt.title('Validation Curve (Biasâ€“Variance Tradeoff)')
plt.legend()
plt.show()

# visualizing the Decision Tree Classifier model
from sklearn.tree import plot_tree

plt.figure(figsize=(40,10))

features_names_after_preprocessing = steps.named_steps['preprocessor'].get_feature_names_out().tolist()

plot_tree(steps.named_steps['model'],
          feature_names=features_names_after_preprocessing,
          class_names=['Not Canceled', 'Not_Canceled'],
          filled=True,
          rounded=True,
          max_depth=8)

plt.title('Decision Tree Visualization (Max Depth 8)')
plt.show()

# Feature Importance
importances_rf = pd.Series(steps.named_steps['model'].feature_importances_, index = features_names_after_preprocessing).sort_values()
ax = importances_rf.plot(kind='barh', color='lightgreen')
ax.bar_label(ax.containers[0], fmt='%.2f')
plt.title('Feature Importance')
plt.show()

"""#### Random Forest"""

# Intantiate a rondom forests classification
steps_rf = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', RandomForestClassifier(n_estimators=200, min_samples_leaf=50, class_weight='balanced', random_state=20))])

steps_rf.fit(X_train, y_train)

# predict the test dataset using Random Forest bagging
y_pred_rf = steps_rf.predict(X_test)

# Checking the confusion matrix of Random Forest using test dataset
cm_train_rf = confusion_matrix(y_test, y_pred_rf)
sns.heatmap(cm_train_rf, annot=True, fmt='g')
plt.xlabel('Predicted label')
plt.ylabel('Actual label')

# Checking the accuracy score of the Random Forest using the test dataset
acc_test_rf = accuracy_score(y_test, y_pred_rf)
print(f"Accuracy test: {acc_test_rf}")

# Checking the accuracy score of the Random Forest using the training dataset
acc_train_rf = accuracy_score(y_train, steps_rf.predict(X_train))
print(f"Accuracy train: {acc_train_rf}")

# Checking the recall score of the Random Forest using the test and training dataset
print(recall_score(y_test, steps_rf.predict(X_test)))
print(recall_score(y_train, steps_rf.predict(X_train)))

# Cross Validation score of the Random Forest
CV_rf_scores = cross_val_score(steps_rf, X, y, scoring='average_precision', cv=10, n_jobs=-1)
print(f'Cross Validation score: {CV_rf_scores.mean()}')

# PR-AUC score of the Random Forest
pr_auc_rf = average_precision_score(y_test, steps_rf.predict(X_test))
print(pr_auc_rf)

# Feature Importances of the Random Forest model
importances_rf = pd.Series(steps_rf.named_steps['model'].feature_importances_, index = features_names_after_preprocessing).sort_values()
ax = importances_rf.plot(kind='barh', color='lightgreen')
ax.bar_label(ax.containers[0], fmt='%.2f')
plt.title('Feature Importance')
plt.show()

"""#### Gradient Boosting"""

# Importing Gradient Boosting from Scikit Learning library
from sklearn.ensemble import GradientBoostingClassifier

# Intantiate gradient boosting
steps_gbt = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', GradientBoostingClassifier(n_estimators=300, max_depth=4, learning_rate=0.05, min_samples_leaf=30, subsample=0.8,random_state=20))])

steps_gbt.fit(X_train, y_train)

# Predicting the outcome of the test dataset using Gradient Boosting
y_pred_gbt = steps_gbt.predict(X_test)

# Showing confusion matrix of Gradient Boosting using the test dataset
cm_train_gbt = confusion_matrix(y_test, y_pred_gbt)
sns.heatmap(cm_train_gbt, annot=True, fmt='g')
plt.xlabel('Predicted label')
plt.ylabel('Actual label')

# Checking the accuracy score of the Gradient Boosting using the test dataset
acc_test_gbt = accuracy_score(y_test, y_pred_gbt)
print(f"Accuracy test: {acc_test_gbt}")

# Checking the accuracy score of the Gradient Boosting using the training dataset
acc_train_gbt = accuracy_score(y_train, steps_gbt.predict(X_train))
print(f"Accuracy train: {acc_train_gbt}")

# Checking the recalll score of the Gradient Boosting using the test and training dataset
print(recall_score(y_test, steps_gbt.predict(X_test)))
print(recall_score(y_train, steps_gbt.predict(X_train)))

# Checking the cross validation score of the Gradient Boosting
CV_gbt_scores = cross_val_score(steps_gbt, X, y, scoring='average_precision', cv=10, n_jobs=-1)
print(f'Cross Validation score of Gradient Boosting: {CV_gbt_scores.mean()}')

# PR-AUC score of the Gradient Boosting
pr_auc_gbt = average_precision_score(y_test, steps_gbt.predict(X_test))
print(pr_auc_gbt)

# Feature Importance of the Gradient Boosting
importances_rf = pd.Series(steps_gbt.named_steps['model'].feature_importances_, index = features_names_after_preprocessing).sort_values()
ax = importances_rf.plot(kind='barh', color='lightgreen')
ax.bar_label(ax.containers[0], fmt='%.2f')
plt.title('Feature Importance')
plt.show()

"""#### Model Tuning using GridSearch CV"""

# Import GridSearch CV
from sklearn.model_selection import GridSearchCV

# Define the grid of hyperparameters steps_gbt
params_gbt = {
    'model__max_depth': [3, 4],
    'model__learning_rate': [0.05, 0.1],
    'model__n_estimators': [200, 400]
}

# Instantiate a 10-fold CV grid search object steps_gbt
grid_gbt = GridSearchCV(estimator=steps_gbt,
                        param_grid=params_gbt,
                        scoring='average_precision',
                        cv=5,
                        n_jobs=-1)

# Fit the grid search object
grid_gbt.fit(X_train, y_train)

# Extract best hyperparameters from grid_gbt
best_hyperparams = grid_gbt.best_params_
print('Best hyperparameters:', best_hyperparams)

# Extract best CV score from grid_gbt
best_CV_score = grid_gbt.best_score_
print('Best CV score:', best_CV_score)

# Extract best model from grid_gbt
best_model = grid_gbt.best_estimator_
y_proba_grid = best_model.predict_proba(X_test)[:, 1]
y_pred_grid = best_model.predict(X_test)
pr_auc_grid = average_precision_score(y_test, y_proba_grid)
print('Test set PR-AUC:', pr_auc_grid)

recall_grid = recall_score(y_test, y_pred_grid)
print('Test set recall:', recall_grid)

# Confusion matrix of the GridSearch CV of the best estimator using the test dataset
cm_train_grid_gbt = confusion_matrix(y_test, grid_gbt.best_estimator_.predict(X_test))
sns.heatmap(cm_train_grid_gbt, annot=True, fmt='g')
plt.xlabel('Predicted label')
plt.ylabel('Actual label')

# Accuracy score of GridSearch CV using the best estimator parameter of the test and training dataset
acc_test_grid_gbt = accuracy_score(y_test, grid_gbt.best_estimator_.predict(X_test))
print(f"Accuracy test: {acc_test_grid_gbt}")

acc_train_grid_gbt = accuracy_score(y_train, grid_gbt.best_estimator_.predict(X_train))
print(f"Accuracy train: {acc_train_grid_gbt}")

# Recall score of GridSearch CV using the best estimator parameter of the test and training dataset
acc_test_grid_gbt = recall_score(y_test, grid_gbt.best_estimator_.predict(X_test))
print(f"Accuracy test: {acc_test_grid_gbt}")

acc_train_grid_gbt = recall_score(y_train, grid_gbt.best_estimator_.predict(X_train))
print(f"Accuracy train: {acc_train_grid_gbt}")

# Feature Importances of the GridSearch CV using the best model parameter
importances_rf = pd.Series(best_model.named_steps['model'].feature_importances_, index = features_names_after_preprocessing).sort_values()
ax = importances_rf.plot(kind='barh', color='lightgreen')
ax.bar_label(ax.containers[0], fmt='%.2f')
plt.title('Feature Importance')
plt.show()